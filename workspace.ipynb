{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57111373",
   "metadata": {},
   "source": [
    "## Drone Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential imports\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# plotting and display\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# YOLO model from ultralytics package\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# custom utilities\n",
    "from utils import *\n",
    "\n",
    "# reflect changes in src code immediately w/o restarting kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# path to all datasets\n",
    "data_path = './data/drone-tracking-datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc88bd9",
   "metadata": {},
   "source": [
    "#### Set device and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device depending on available GPU\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model size: n for nano, s for small, m for medium, l for large, x for extra large\n",
    "model_size = 'm'  \n",
    "\n",
    "# load pretrained YOLOv8 model\n",
    "model = YOLO(f\"yolov8{model_size}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7a418",
   "metadata": {},
   "source": [
    "#### Select dataset and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aee16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataset and camera\n",
    "dataset_num = 3\n",
    "cam_num = 4\n",
    "video_path = os.path.join(data_path, f'dataset{dataset_num}/cam{cam_num}.mp4')\n",
    "\n",
    "# get sample frame and store width and height\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, sample_frame = cap.read()\n",
    "cap.release()\n",
    "sample_frame_rgb = cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB)\n",
    "frame_height, frame_width, _ = sample_frame.shape\n",
    "print(f\"Frame size: {frame_width} x {frame_height}\")\n",
    "plt.imshow(sample_frame_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834036dc",
   "metadata": {},
   "source": [
    "#### Run detection with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select tracker\n",
    "tracker = \"sort.yaml\"  # or 'bytetrack.yaml', 'strongsort.yaml'\n",
    "\n",
    "# pick either show or stream\n",
    "\n",
    "# run detection on video\n",
    "results = model.predict(\n",
    "    source=video_path, \n",
    "    device=device,\n",
    "    tracker=tracker,\n",
    "\n",
    "    #show=True, # display live output in external window (do not return generator)\n",
    "    save=True,  # save output video with bounding boxes to runs/detect/\n",
    "    stream=True  # return a generator that yields results for each frame, doesn't store whole video in memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43738ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# haven't finetuned YOLO yet for drone detection class \n",
    "# in the meantime, model identifies drones as either airplanes, birds, kites\n",
    "print(model.names[4])\n",
    "print(model.names[14])\n",
    "print(model.names[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4200d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep detections of target classes (airplane, bird, kite in that order)\n",
    "target_classes = torch.tensor([4, 14, 33], device=device, dtype=torch.float32)\n",
    "\n",
    "# save trajectory to CSV\n",
    "csv_file = f\"trajectory_dataset{dataset_num}_cam{cam_num}.csv\"\n",
    "\n",
    "trajectory = []  # list to store (frame_id, x_center, y_center)\n",
    "\n",
    "for frame_id, result in enumerate(results, start=1):\n",
    "\n",
    "    # filter boxes to only target classes\n",
    "    mask = torch.isin(result.boxes.cls, target_classes)\n",
    "    target_boxes = result.boxes[mask]\n",
    "    \n",
    "    if len(target_boxes) > 0:\n",
    "        # Choose the most confident detection among these classes\n",
    "        best_idx = target_boxes.conf.argmax()\n",
    "        box = target_boxes[best_idx].xywh[0]\n",
    "        x_center, y_center = box[0].item(), box[1].item()\n",
    "    else:\n",
    "        x_center, y_center = float('nan'), float('nan')\n",
    "\n",
    "    trajectory.append([frame_id, x_center, y_center])\n",
    "\n",
    "# save to CSV\n",
    "with open(csv_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"frame_id\", \"x_center\", \"y_center\"])\n",
    "    writer.writerows(trajectory)\n",
    "\n",
    "print(f\"Trajectory saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f718642",
   "metadata": {},
   "source": [
    "#### Plot 2D trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_trajectory(trajectory, sample_frame_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812390c",
   "metadata": {},
   "source": [
    "#### Plot GT 3D trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file\n",
    "data_path = './data/drone-tracking-datasets/'\n",
    "dataset_num = 3\n",
    "trajectory_path = os.path.join(data_path, f'dataset{dataset_num}/trajectory/rtk.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_trajectory_static(trajectory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c63775",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_trajectory_interactive(trajectory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c8a04",
   "metadata": {},
   "source": [
    "#### Dataset 3 - Camera Network Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# positions of cameras in world coordinates\n",
    "''' \n",
    "44.535 11.56253333 -1.1467\n",
    "4.3962 -54.27346667 4.0965\n",
    "-42.5242 -21.00086667 -1.7639\n",
    "40.1912 44.79053333 -0.9379\n",
    "-34.8458 -44.00906667 1.4179\n",
    "-11.7524 62.93033333 -1.6659\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ff81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataset\n",
    "dataset_num = 3\n",
    "\n",
    "# get camera types\n",
    "camera_names_path = os.path.join(data_path, f'dataset{dataset_num}/cameras.txt')\n",
    "camera_names = get_camera_names(camera_names_path)\n",
    "print(camera_names)\n",
    "\n",
    "# load camera positions (x,y,z)\n",
    "cam_pos = np.loadtxt(os.path.join(data_path, f'dataset{dataset_num}/camera-locations/campos.txt'))\n",
    "\n",
    "\n",
    "# synchronization parameters between cameras:\n",
    "\n",
    "# time scale (alpha) matrix\n",
    "alpha_matrix = np.array([\n",
    "    [1.0000, 0.5005, 0.4960, 0.4171, 0.5000, 0.8341],\n",
    "    [1.9982, 1.0000, 0.9910, 0.8333, 0.9990, 1.6667],\n",
    "    [2.0163, 1.0091, 1.0000, 0.8409, 1.0081, 1.6819],\n",
    "    [2.3978, 1.2000, 1.1892, 1.0000, 1.1988, 2.0000],\n",
    "    [2.0001, 1.0010, 0.9919, 0.8342, 1.0000, 1.6683],\n",
    "    [1.1989, 0.6000, 0.5946, 0.5000, 0.5994, 1.0000],\n",
    "])\n",
    "\n",
    "# time shift (beta) matrix\n",
    "beta_matrix = np.array([\n",
    "    [0.00,     1013.95,  546.98,  251.16,  961.02,  137.51],\n",
    "    [-2026.04,    0.00, -457.83, -593.82,  -51.96, -1552.47],\n",
    "    [-1102.90,  461.99,    0.00, -208.81,  409.59,  -782.45],\n",
    "    [ -602.21,  712.57,  248.32,    0.00,  659.93,  -364.81],\n",
    "    [-1922.12,   52.01, -406.29, -551.00,    0.00, -1465.78],\n",
    "    [ -164.85,  931.45,  465.22,  182.40,  878.60,     0.00],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intrinsics for the cameras from json files\n",
    "camera_name = 'gopro3'\n",
    "intrinsics_dict = get_camera_intrinsics(camera_name) \n",
    "\n",
    "print(intrinsics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f56875",
   "metadata": {},
   "source": [
    "#### Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c500ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select reference cam\n",
    "ref_cam_num = 0\n",
    "\n",
    "# isolate relevant rows for sync\n",
    "alpha_ref = alpha_matrix[ref_cam_num, :]\n",
    "beta_ref = beta_matrix[ref_cam_num, :]\n",
    "\n",
    "print(alpha_ref)\n",
    "print(beta_ref) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide reference camera\n",
    "# get its data\n",
    "\n",
    "# the loop through other cameras and get their point correspondences\n",
    "# synchronized and undistorted\n",
    "# then we have variables for all the point pairs\n",
    "\n",
    "# total number of cameras\n",
    "total_cams = len(camera_names)\n",
    "\n",
    "# list of total_cams-1 lists with correspondences (each lists is Nx4 with x1,y1,x2,y2)\n",
    "all_matches = []\n",
    "\n",
    "# select reference cam\n",
    "ref_cam_num = 0\n",
    "\n",
    "# isolate relevant rows for sync\n",
    "alpha_ref = alpha_matrix[ref_cam_num, :]\n",
    "beta_ref = beta_matrix[ref_cam_num, :]\n",
    "\n",
    "# get detection data for reference cam: frame_id, x_center, y_center \n",
    "ref_detections_path = os.path.join(data_path, f'dataset{dataset_num}/detections/cam{ref_cam_num}.txt')\n",
    "ref_detections = np.loadtxt(ref_detections_path, skiprows=1)\n",
    "\n",
    "# load intrinsics for reference\n",
    "intrinsics_ref = get_camera_intrinsics(camera_names[ref_cam_num])\n",
    "K1 = np.array(intrinsics_ref['K'])\n",
    "dist1 = np.array(intrinsics_ref['dist_coeff'])\n",
    "\n",
    "\n",
    "# iterate through all other cameras to get synchronized points\n",
    "for cam_idx in range(1, total_cams):\n",
    "\n",
    "    # get detection data for cur camera\n",
    "    pathname = os.path.join(data_path, f'dataset{dataset_num}/detections/cam{cam_idx}.txt')\n",
    "    detection_data = np.loadtxt(pathname, skiprows=1)\n",
    "\n",
    "    # get sync params for cur camera\n",
    "    alpha = alpha_ref[cam_idx]\n",
    "    beta = beta_ref[cam_idx]\n",
    "\n",
    "    # collect points\n",
    "    cur_pts = []\n",
    "\n",
    "    # iterate through all frames of reference camera\n",
    "    for i in range(len(ref_detections)):\n",
    "\n",
    "        # use sync info to get corresponding frame id\n",
    "        j = alpha * i + beta\n",
    "        j = int(round(j))\n",
    "        if j < 0 or j >= len(detection_data):\n",
    "            continue  # skip if out of bounds\n",
    "\n",
    "        # get x, y coords from both cameras\n",
    "        x1, y1 = ref_detections[i, 1], ref_detections[i, 2]\n",
    "        x2, y2 = detection_data[j, 1], detection_data[j, 2]\n",
    "\n",
    "        # skip if either coord is (0.0, 0.0)\n",
    "        if (x1 == 0.0 and y1 == 0.0) or (x2 == 0.0 and y2 == 0.0):\n",
    "            continue\n",
    "\n",
    "        # append the correspondence\n",
    "        cur_pts.append([x1, y1, x2, y2])\n",
    "\n",
    "    # append to all points\n",
    "    all_matches.append(cur_pts)\n",
    "\n",
    "\n",
    "# for dataset3 there are 6 cameras\n",
    "# the all_matches list has 5 lists that match the reference camera points to all other cams\n",
    "print(len(all_matches))\n",
    "print(len(all_matches[0]))\n",
    "print(len(all_matches[1]))\n",
    "print(len(all_matches[2]))\n",
    "print(len(all_matches[3]))\n",
    "print(len(all_matches[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to triangulate each camera to the reference cam\n",
    "\n",
    "for i in range(len(all_matches)):\n",
    "\n",
    "    # load intrinsics\n",
    "    intrinsics = get_camera_intrinsics(camera_names[i+1])\n",
    "    K2 = np.array(intrinsics_ref['K'])\n",
    "    dist2 = np.array(intrinsics_ref['dist_coeff'])\n",
    "\n",
    "    # get current matches\n",
    "    match_list = all_matches[i]\n",
    "    match_list = np.array(match_list)\n",
    "    print(match_list.shape)\n",
    "    \n",
    "    # split the list into 2\n",
    "    pts1, pts2 = match_list[:,:2], match_list[:,2:4]\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    pts1 = np.array(pts1, dtype=np.float32)\n",
    "    pts2 = np.array(pts2, dtype=np.float32)\n",
    "\n",
    "    # undistort points with cv2 method\n",
    "    pts1_undist = cv2.undistortPoints(pts1, K1, dist1)\n",
    "    pts2_undist = cv2.undistortPoints(pts2, K2, dist2)\n",
    "\n",
    "    # estimate essential matrix E using all (undistorted) point correspondences\n",
    "    E, inlier_mask = cv2.findEssentialMat(pts1_undist, pts2_undist, focal=1.0, pp=(0., 0.), method=cv2.RANSAC, prob=0.999, threshold=1e-3)\n",
    "\n",
    "    # filter points by inlier mask\n",
    "    pts1_undist = pts1_undist[inlier_mask.ravel() == 1]\n",
    "    pts2_undist = pts2_undist[inlier_mask.ravel() == 1]\n",
    "\n",
    "    # recover rotation R and translation t\n",
    "    _, R, t, mask_pose = cv2.recoverPose(E, pts1_undist, pts2_undist)\n",
    "\n",
    "    # build projection matrices\n",
    "    P1 = np.hstack((np.eye(3), np.zeros((3, 1))))  # first camera at origin\n",
    "    P2 = np.hstack((R, t))  # second camera with R, t relative to first\n",
    "\n",
    "    # triangulate 3D points\n",
    "    points_4d_h = cv2.triangulatePoints(P1, P2, pts1_undist.reshape(2, -1), pts2_undist.reshape(2, -1))\n",
    "    points_3d = (points_4d_h[:3, :] / points_4d_h[3, :]).T  # convert from homogeneous\n",
    "    points_3d = np.array(points_3d)\n",
    "\n",
    "    # filter points with positive depth (in front of cameras)\n",
    "    valid = points_3d[:,2] > 0\n",
    "    points_3d = points_3d[valid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWO CAMERAS!!!!!\n",
    "\n",
    "\n",
    "# first lets try the triangulation on the ground truth detections\n",
    "# and double check if the frames align\n",
    "\n",
    "# maybe start with 2 cams\n",
    "cam_A = 0\n",
    "cam_B = 1\n",
    "\n",
    "# load intrinsics\n",
    "intrinsics_A = get_camera_intrinsics(camera_names[cam_A])\n",
    "intrinsics_B = get_camera_intrinsics(camera_names[cam_B])\n",
    "K1 = np.array(intrinsics_A['K'])\n",
    "dist1 = np.array(intrinsics_A['dist_coeff'])\n",
    "K2 = np.array(intrinsics_B['K'])\n",
    "dist2 = np.array(intrinsics_B['dist_coeff'])\n",
    "\n",
    "# get detection data: frame_id, x_center, y_center \n",
    "data_A_path = os.path.join(data_path, f'dataset{dataset_num}/detections/cam{cam_A}.txt')\n",
    "data_B_path = os.path.join(data_path, f'dataset{dataset_num}/detections/cam{cam_B}.txt')\n",
    "data_A = np.loadtxt(data_A_path, skiprows=1)\n",
    "data_B = np.loadtxt(data_B_path, skiprows=1)\n",
    "\n",
    "# get sync params\n",
    "alpha = alpha_matrix[cam_A, cam_B]\n",
    "beta = beta_matrix[cam_A, cam_B]\n",
    "\n",
    "# collect corresponding pairs of points\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "# iterate through frames of reference camera\n",
    "for frame_id in range(len(data_A)):\n",
    "\n",
    "    # use sync info to get corresponding frame id from other camera\n",
    "    match_id = alpha * frame_id + beta\n",
    "    match_id = int(round(match_id))\n",
    "    if match_id < 0 or match_id >= len(data_B):\n",
    "        continue  # skip if out of bounds\n",
    "\n",
    "    # get x, y coords from both cameras\n",
    "    x1, y1 = data_A[frame_id, 1], data_A[frame_id, 2]\n",
    "    x2, y2 = data_B[match_id, 1], data_B[match_id, 2]\n",
    "\n",
    "    # skip if either point in the pair is 0.0\n",
    "    if x1 == 0.0 or y1 == 0.0 or x2 == 0.0 or y2 == 0.0:\n",
    "        continue\n",
    "\n",
    "    pts1.append([x1, y1])\n",
    "    pts2.append([x2, y2])\n",
    "\n",
    "# convert to numpy arrays\n",
    "pts1 = np.array(pts1, dtype=np.float32)\n",
    "pts2 = np.array(pts2, dtype=np.float32)\n",
    "\n",
    "\n",
    "# undistort points with cv2 method\n",
    "pts1_undist = cv2.undistortPoints(pts1, K1, dist1, P=None)\n",
    "pts2_undist = cv2.undistortPoints(pts2, K2, dist2, P=None)\n",
    "\n",
    "# look into this??? P variable, dont know if points should be normalized or not\n",
    "# with no projection matrix P, points are normalized\n",
    "# with P=K, points are in pixel coordinates\n",
    "\n",
    "\n",
    "# estimate essential matrix E using all (undistorted) point correspondences\n",
    "# select threshold parameter according to pixel or normalized\n",
    "E, inlier_mask = cv2.findEssentialMat(pts1_undist, pts2_undist, focal=1.0, pp=(0., 0.), method=cv2.RANSAC, prob=0.999, threshold=1e-3)\n",
    "# focal=1.0, pp=(0., 0.)\n",
    "\n",
    "\n",
    "\n",
    "# filter points by inlier mask\n",
    "pts1_undist = pts1_undist[inlier_mask.ravel() == 1]\n",
    "pts2_undist = pts2_undist[inlier_mask.ravel() == 1]\n",
    "\n",
    "\n",
    "# recover rotation R and translation t\n",
    "_, R, t, mask_pose = cv2.recoverPose(E, pts1_undist, pts2_undist)\n",
    "\n",
    "# build projection matrices\n",
    "P1 = np.hstack((np.eye(3), np.zeros((3, 1))))  # first camera at origin\n",
    "P2 = np.hstack((R, t))  # second camera with R, t relative to first\n",
    "\n",
    "# triangulate 3D points\n",
    "points_4d_h = cv2.triangulatePoints(P1, P2, pts1_undist.reshape(2, -1), pts2_undist.reshape(2, -1))\n",
    "points_3d = (points_4d_h[:3, :] / points_4d_h[3, :]).T  # convert from homogeneous\n",
    "points_3d = np.array(points_3d)\n",
    "\n",
    "# filter points with positive depth (in front of cameras)\n",
    "valid = points_3d[:,2] > 0\n",
    "points_3d = points_3d[valid]\n",
    "\n",
    "# save to text file\n",
    "np.savetxt(\"output.txt\", points_3d)\n",
    "\n",
    "print(f\"Triangulated {points_3d.shape[0]} points.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform points to world coordinates\n",
    "# if we had the rotation and translation of cam_A in world coords\n",
    "\n",
    "# Example: known world camera centers\n",
    "C1_world = np.array(cam_pos[cam_A])\n",
    "C2_world = np.array(cam_pos[cam_B])\n",
    "\n",
    "# From your recovered pose\n",
    "t = t.reshape(3)\n",
    "C1_recon = np.zeros(3)\n",
    "C2_recon = t  # because in your triangulation, first cam is at origin\n",
    "\n",
    "# Compute distances\n",
    "d_real = np.linalg.norm(C2_world - C1_world)\n",
    "d_recon = np.linalg.norm(C2_recon)\n",
    "\n",
    "# Compute scale\n",
    "scale = d_real / d_recon\n",
    "\n",
    "# Apply scale to everything\n",
    "t_scaled = t * scale\n",
    "points_3d_scaled = points_3d * scale\n",
    "\n",
    "print(points_3d_scaled.shape)\n",
    "\n",
    "# save to text file\n",
    "np.savetxt(\"output_scaled.txt\", points_3d_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defo a way to vectorize this for all points\n",
    "\n",
    "# need to be consistent between normalized and pixel coords\n",
    "\n",
    "3589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the 3d points\n",
    "trajectory_path = \"output_scaled.txt\"\n",
    "plot_3d_trajectory_interactive(trajectory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CSV has header\n",
    "data_1 = np.genfromtxt(\"trajectory.csv\", delimiter=\",\", skip_header=1)\n",
    "data_2 = np.genfromtxt(\"trajectory_1.csv\", delimiter=\",\", skip_header=1)\n",
    "\n",
    "# chose second camera\n",
    "cam_num_2 = 1\n",
    "# get sync params\n",
    "alpha_2 = alpha_ref[cam_num_2]\n",
    "beta_2 = beta_ref[cam_num_2]\n",
    "\n",
    "\n",
    "# iterate through frames of reference camera\n",
    "for frame_id in range(len(data)):\n",
    "\n",
    "    # use sync info to get corresponding frame id from other camera\n",
    "    j = alpha_2 * frame_id + beta_2\n",
    "    if j < 0 or j >= len(data_2):\n",
    "        continue  # skip if out of bounds\n",
    "\n",
    "    # get x, y coords from both cameras\n",
    "    x1, y1 = data_1[frame_id, 1], data_1[frame_id, 2]\n",
    "    x2, y2 = data_2[int(j), 1], data_2[int(j), 2]\n",
    "\n",
    "    # undistort points using cv2 method\n",
    "    ray1 = cv2.undistortPoints(np.array([[[x1, y1]]], dtype=np.float32), K1, dist1, P=K1)\n",
    "    ray2 = cv2.undistortPoints(np.array([[[x2, y2]]], dtype=np.float32), K2, dist2, P=K2)\n",
    "\n",
    "    # transform rays to world coordinates\n",
    "    \n",
    "\n",
    "    # triangulate 3d point from rays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fa70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to match the points in two overlapping frames\n",
    "\n",
    "data_path = './data/drone-tracking-datasets/'\n",
    "\n",
    "def get_first_frame(video_path):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, sample_frame = cap.read()\n",
    "    cap.release()\n",
    "    sample_frame_rgb = cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB)\n",
    "    #plt.imshow(sample_frame_rgb)\n",
    "    #plt.show()\n",
    "\n",
    "    return sample_frame_rgb\n",
    "\n",
    "\n",
    "video_path = os.path.join(data_path, f'dataset3/cam1.mp4')\n",
    "cam1_frame = get_first_frame(video_path)\n",
    "\n",
    "video_path = os.path.join(data_path, f'dataset3/cam2.mp4')\n",
    "cam2_frame = get_first_frame(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafe0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cam1_frame.shape)\n",
    "print(cam2_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to the fundamentals\n",
    "\n",
    "# matched points in pixel coordinates\n",
    "pts1 = np.array([...], dtype=np.float32)  # Nx2\n",
    "pts2 = np.array([...], dtype=np.float32)  # Nx2\n",
    "\n",
    "# camera intrinsics\n",
    "K1 = np.array([[fx1, 0, cx1],\n",
    "               [0, fy1, cy1],\n",
    "               [0,   0,   1]], dtype=np.float64)\n",
    "K2 = np.array([[fx2, 0, cx2],\n",
    "               [0, fy2, cy2],\n",
    "               [0,   0,   1]], dtype=np.float64)\n",
    "\n",
    "# normalize points\n",
    "pts1_norm = cv2.undistortPoints(pts1.reshape(-1,1,2), K1, None)\n",
    "pts2_norm = cv2.undistortPoints(pts2.reshape(-1,1,2), K2, None)\n",
    "\n",
    "# compute essential matrix directly\n",
    "E, mask = cv2.findEssentialMat(pts1_norm, pts2_norm, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "\n",
    "# recover pose (rotation, translation)\n",
    "_, R, t, mask_pose = cv2.recoverPose(E, pts1_norm, pts2_norm)\n",
    "\n",
    "print(\"Rotation:\\n\", R)\n",
    "print(\"Translation direction:\\n\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d62179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the detection on all videos and save to file\n",
    "data_path = './data/drone-tracking-datasets/'\n",
    "dataset_num = 3\n",
    "num_cams = 6\n",
    "\n",
    "# select tracker\n",
    "tracker = \"sort.yaml\"  # or 'bytetrack.yaml', 'strongsort.yaml'\n",
    "\n",
    "for cam_num in range(num_cams):\n",
    "    video_path = os.path.join(data_path, f'dataset{dataset_num}/cam{cam_num}.mp4')\n",
    "    csv_file = os.path.join('dataset3_result_detections', f'detections_cam{cam_num}.txt')\n",
    "\n",
    "    results = model.predict(source=video_path, tracker=tracker, save=False, stream=True)\n",
    "\n",
    "    trajectory = []  # list to store (frame_id, x_center, y_center)\n",
    "\n",
    "    for frame_id, result in enumerate(results, start=1):\n",
    "        if len(result.boxes) > 0:\n",
    "            # Take first detection (assuming single drone)\n",
    "            box = result.boxes[0].xywh[0]  # [x_center, y_center, width, height] in pixels\n",
    "            x_center, y_center = box[0].item(), box[1].item()\n",
    "        else:\n",
    "            # If no detection, log 0\n",
    "            x_center, y_center = 0.0, 0.0\n",
    "\n",
    "        trajectory.append([frame_id, x_center, y_center])\n",
    "\n",
    "    # save to CSV\n",
    "    with open(csv_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"frame_id\", \"x_center\", \"y_center\"])\n",
    "        writer.writerows(trajectory)\n",
    "\n",
    "    print(f\"Trajectory saved to {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone-detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
