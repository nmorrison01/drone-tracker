{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57111373",
   "metadata": {},
   "source": [
    "## Drone Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential imports\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting and display\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# YOLO model from ultralytics package\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# custom utilities\n",
    "from utils import *\n",
    "\n",
    "# reflect changes in src code immediately w/o restarting kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# path to all datasets\n",
    "data_path = './data/drone-tracking-datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc88bd9",
   "metadata": {},
   "source": [
    "#### Set device and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device depending on available GPU\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model size: n for nano, s for small, m for medium, l for large, x for extra large\n",
    "model_size = 'm'  \n",
    "\n",
    "# load pretrained YOLOv8 model\n",
    "model = YOLO(f\"yolov8{model_size}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7a418",
   "metadata": {},
   "source": [
    "#### Select dataset and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aee16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataset and camera\n",
    "dataset_num = 1\n",
    "cam_num = 3\n",
    "video_path = os.path.join(data_path, f'dataset{dataset_num}/cam{cam_num}.mp4')\n",
    "\n",
    "# get sample frame and store width and height\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, sample_frame = cap.read()\n",
    "cap.release()\n",
    "sample_frame_rgb = cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB)\n",
    "frame_height, frame_width, _ = sample_frame.shape\n",
    "print(f\"Frame size: {frame_width} x {frame_height}\")\n",
    "plt.imshow(sample_frame_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834036dc",
   "metadata": {},
   "source": [
    "#### Run detection with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select tracker\n",
    "tracker = \"sort.yaml\"  # or 'bytetrack.yaml', 'strongsort.yaml'\n",
    "\n",
    "# PICK either show or stream (comment other out)\n",
    "\n",
    "# run detection on video\n",
    "results = model.predict(\n",
    "    source=video_path, \n",
    "    device=device,\n",
    "    tracker=tracker,\n",
    "\n",
    "    #show=True, # display live output in external window (do not return generator)\n",
    "    save=True,  # save output video with bounding boxes to runs/detect/\n",
    "    stream=True  # return a generator that yields results for each frame, doesn't store whole video in memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43738ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# haven't finetuned YOLO yet for drone detection class \n",
    "# in the meantime, model identifies drones as either airplanes, birds, kites\n",
    "print(model.names[4])\n",
    "print(model.names[14])\n",
    "print(model.names[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4200d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep detections of target classes (airplane, bird, kite in that order)\n",
    "target_classes = torch.tensor([4, 14, 33], device=device, dtype=torch.float32)\n",
    "\n",
    "# save trajectory to CSV\n",
    "csv_file = f\"./trajectories_2d/trajectory_dataset{dataset_num}_cam{cam_num}.csv\"\n",
    "\n",
    "trajectory = []  # list to store (frame_id, x_center, y_center)\n",
    "\n",
    "for frame_id, result in enumerate(results, start=1):\n",
    "\n",
    "    # filter boxes to only target classes\n",
    "    mask = torch.isin(result.boxes.cls, target_classes)\n",
    "    target_boxes = result.boxes[mask]\n",
    "    \n",
    "    if len(target_boxes) > 0:\n",
    "        # Choose the most confident detection among these classes\n",
    "        best_idx = target_boxes.conf.argmax()\n",
    "        box = target_boxes[best_idx].xywh[0]\n",
    "        x_center, y_center = box[0].item(), box[1].item()\n",
    "    else:\n",
    "        x_center, y_center = float('nan'), float('nan')\n",
    "\n",
    "    trajectory.append([frame_id, x_center, y_center])\n",
    "\n",
    "# save to CSV\n",
    "with open(csv_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"frame_id\", \"x_center\", \"y_center\"])\n",
    "    writer.writerows(trajectory)\n",
    "\n",
    "print(f\"Trajectory saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f718642",
   "metadata": {},
   "source": [
    "#### Plot 2D trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trajectory_path = f\"./trajectories_2d/trajectory_dataset{dataset_num}_cam{cam_num}.csv\"\n",
    "plot_2d_trajectory(trajectory, sample_frame_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812390c",
   "metadata": {},
   "source": [
    "#### Plot GT 3D trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file\n",
    "data_path = './data/drone-tracking-datasets/'\n",
    "dataset_num = 1\n",
    "trajectory_path = os.path.join(data_path, f'dataset{dataset_num}/trajectory/rtk.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_trajectory_static(trajectory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c63775",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_trajectory_interactive(trajectory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c8a04",
   "metadata": {},
   "source": [
    "#### Dataset 3 - Camera Network Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ff81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataset\n",
    "dataset_num = 3\n",
    "\n",
    "# get camera types\n",
    "camera_names_path = os.path.join(data_path, f'dataset{dataset_num}/cameras.txt')\n",
    "camera_names = get_camera_names(camera_names_path)\n",
    "print(camera_names)\n",
    "\n",
    "# load camera positions (x,y,z) in world coords\n",
    "cam_pos = np.loadtxt(os.path.join(data_path, f'dataset{dataset_num}/camera-locations/campos.txt'))\n",
    "\n",
    "\n",
    "# synchronization parameters between cameras:\n",
    "\n",
    "# time scale (alpha) matrix\n",
    "alpha_matrix = np.array([\n",
    "    [1.0000, 0.5005, 0.4960, 0.4171, 0.5000, 0.8341],\n",
    "    [1.9982, 1.0000, 0.9910, 0.8333, 0.9990, 1.6667],\n",
    "    [2.0163, 1.0091, 1.0000, 0.8409, 1.0081, 1.6819],\n",
    "    [2.3978, 1.2000, 1.1892, 1.0000, 1.1988, 2.0000],\n",
    "    [2.0001, 1.0010, 0.9919, 0.8342, 1.0000, 1.6683],\n",
    "    [1.1989, 0.6000, 0.5946, 0.5000, 0.5994, 1.0000],\n",
    "])\n",
    "\n",
    "# time shift (beta) matrix\n",
    "beta_matrix = np.array([\n",
    "    [0.00,     1013.95,  546.98,  251.16,  961.02,  137.51],\n",
    "    [-2026.04,    0.00, -457.83, -593.82,  -51.96, -1552.47],\n",
    "    [-1102.90,  461.99,    0.00, -208.81,  409.59,  -782.45],\n",
    "    [ -602.21,  712.57,  248.32,    0.00,  659.93,  -364.81],\n",
    "    [-1922.12,   52.01, -406.29, -551.00,    0.00, -1465.78],\n",
    "    [ -164.85,  931.45,  465.22,  182.40,  878.60,     0.00],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the intrinsics for the cameras from json files\n",
    "camera_name = 'gopro3'\n",
    "intrinsics_dict = get_camera_intrinsics(camera_name) \n",
    "\n",
    "print(intrinsics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f56875",
   "metadata": {},
   "source": [
    "#### Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide reference camera\n",
    "# get its data\n",
    "\n",
    "# the loop through other cameras and get their point correspondences\n",
    "# synchronized and undistorted\n",
    "# then we have variables for all the point pairs\n",
    "\n",
    "# total number of cameras\n",
    "total_cams = len(camera_names)\n",
    "\n",
    "# list of total_cams-1 lists with correspondences (each lists is Nx4 with x1,y1,x2,y2)\n",
    "all_matches = []\n",
    "\n",
    "# select reference cam\n",
    "ref_cam_num = 0\n",
    "\n",
    "# isolate relevant rows for sync\n",
    "alpha_ref = alpha_matrix[ref_cam_num, :]\n",
    "beta_ref = beta_matrix[ref_cam_num, :]\n",
    "\n",
    "# get detection data for reference cam: frame_id, x_center, y_center \n",
    "ref_detections_path = os.path.join(data_path, f'dataset{dataset_num}/detections/cam{ref_cam_num}.txt')\n",
    "ref_detections = np.loadtxt(ref_detections_path, skiprows=1)\n",
    "\n",
    "# load intrinsics for reference\n",
    "intrinsics_ref = get_camera_intrinsics(camera_names[ref_cam_num])\n",
    "K1 = np.array(intrinsics_ref['K'])\n",
    "dist1 = np.array(intrinsics_ref['dist_coeff'])\n",
    "\n",
    "\n",
    "# iterate through all other cameras to get synchronized points\n",
    "for cam_idx in range(1, total_cams):\n",
    "\n",
    "    # get detection data for cur camera\n",
    "    pathname = os.path.join(data_path, f'dataset{dataset_num}/detections/cam{cam_idx}.txt')\n",
    "    detection_data = np.loadtxt(pathname, skiprows=1)\n",
    "\n",
    "    # get sync params for cur camera\n",
    "    alpha = alpha_ref[cam_idx]\n",
    "    beta = beta_ref[cam_idx]\n",
    "\n",
    "    # collect points\n",
    "    cur_pts = []\n",
    "\n",
    "    # iterate through all frames of reference camera\n",
    "    for i in range(len(ref_detections)):\n",
    "\n",
    "        # use sync info to get corresponding frame id\n",
    "        j = alpha * i + beta\n",
    "        j = int(round(j))\n",
    "        if j < 0 or j >= len(detection_data):\n",
    "            continue  # skip if out of bounds\n",
    "\n",
    "        # get x, y coords from both cameras\n",
    "        x1, y1 = ref_detections[i, 1], ref_detections[i, 2]\n",
    "        x2, y2 = detection_data[j, 1], detection_data[j, 2]\n",
    "\n",
    "        # skip if either coord is (0.0, 0.0)\n",
    "        if (x1 == 0.0 and y1 == 0.0) or (x2 == 0.0 and y2 == 0.0):\n",
    "            continue\n",
    "\n",
    "        # append the correspondence\n",
    "        cur_pts.append([x1, y1, x2, y2])\n",
    "\n",
    "    # append to all points\n",
    "    all_matches.append(cur_pts)\n",
    "\n",
    "\n",
    "# for dataset3 there are 6 cameras\n",
    "# the all_matches list has 5 lists that match the reference camera points to all other cams\n",
    "print(len(all_matches))\n",
    "print(len(all_matches[0]))\n",
    "print(len(all_matches[1]))\n",
    "print(len(all_matches[2]))\n",
    "print(len(all_matches[3]))\n",
    "print(len(all_matches[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to triangulate each camera to the reference cam\n",
    "\n",
    "world_points = []\n",
    "\n",
    "for i in range(len(all_matches)):\n",
    "\n",
    "    # load intrinsics\n",
    "    intrinsics = get_camera_intrinsics(camera_names[i+1])\n",
    "    K2 = np.array(intrinsics_ref['K'])\n",
    "    dist2 = np.array(intrinsics_ref['dist_coeff'])\n",
    "\n",
    "    # get current matches\n",
    "    match_list = all_matches[i]\n",
    "    match_list = np.array(match_list)\n",
    "    \n",
    "    # split the list into 2\n",
    "    pts1, pts2 = match_list[:,:2], match_list[:,2:4]\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    pts1 = np.array(pts1, dtype=np.float32)\n",
    "    pts2 = np.array(pts2, dtype=np.float32)\n",
    "\n",
    "    # undistort points with cv2 method\n",
    "    pts1_undist = cv2.undistortPoints(pts1, K1, dist1)\n",
    "    pts2_undist = cv2.undistortPoints(pts2, K2, dist2)\n",
    "\n",
    "    # estimate essential matrix E using all (undistorted) point correspondences\n",
    "    E, inlier_mask = cv2.findEssentialMat(pts1_undist, pts2_undist, focal=1.0, pp=(0., 0.), method=cv2.RANSAC, prob=0.999, threshold=1e-3)\n",
    "\n",
    "    # filter points by inlier mask\n",
    "    pts1_undist = pts1_undist[inlier_mask.ravel() == 1]\n",
    "    pts2_undist = pts2_undist[inlier_mask.ravel() == 1]\n",
    "\n",
    "    # recover rotation R and translation t\n",
    "    _, R, t, mask_pose = cv2.recoverPose(E, pts1_undist, pts2_undist)\n",
    "\n",
    "    # build projection matrices\n",
    "    P1 = np.hstack((np.eye(3), np.zeros((3, 1))))  # first camera at origin\n",
    "    P2 = np.hstack((R, t))  # second camera with R, t relative to first\n",
    "\n",
    "    # triangulate 3D points\n",
    "    points_4d_h = cv2.triangulatePoints(P1, P2, pts1_undist.reshape(2, -1), pts2_undist.reshape(2, -1))\n",
    "    points_3d = (points_4d_h[:3, :] / points_4d_h[3, :]).T  # convert from homogeneous\n",
    "    points_3d = np.array(points_3d)\n",
    "\n",
    "    # filter points with positive depth (in front of cameras)\n",
    "    valid = (points_3d[:,2] > 0) & (points_3d[:,2] < 1)\n",
    "    points_3d = points_3d[valid]\n",
    "\n",
    "    print(points_3d.shape)\n",
    "\n",
    "    # add to world points\n",
    "    world_points.append(points_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0035e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack points\n",
    "points_all = np.vstack(world_points)\n",
    "\n",
    "# scale determined using cam_pos\n",
    "scale = 77.28514286510699\n",
    "points_all_scaled = points_all * scale\n",
    "\n",
    "# save to text file\n",
    "trajectory_path = \"multi_cam_output.txt\"\n",
    "np.savetxt(trajectory_path, points_all_scaled)\n",
    "\n",
    "# display the 3d points\n",
    "plot_3d_trajectory_interactive(trajectory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d62179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the detection on all videos and save to file\n",
    "data_path = './data/drone-tracking-datasets/'\n",
    "dataset_num = 3\n",
    "num_cams = 6\n",
    "\n",
    "# select tracker\n",
    "tracker = \"sort.yaml\"  # or 'bytetrack.yaml', 'strongsort.yaml'\n",
    "\n",
    "for cam_num in range(num_cams):\n",
    "    video_path = os.path.join(data_path, f'dataset{dataset_num}/cam{cam_num}.mp4')\n",
    "    csv_file = os.path.join('dataset3_result_detections', f'detections_cam{cam_num}.txt')\n",
    "\n",
    "    results = model.predict(source=video_path, tracker=tracker, save=False, stream=True)\n",
    "\n",
    "    trajectory = []  # list to store (frame_id, x_center, y_center)\n",
    "\n",
    "    for frame_id, result in enumerate(results, start=1):\n",
    "        if len(result.boxes) > 0:\n",
    "            # Take first detection (assuming single drone)\n",
    "            box = result.boxes[0].xywh[0]  # [x_center, y_center, width, height] in pixels\n",
    "            x_center, y_center = box[0].item(), box[1].item()\n",
    "        else:\n",
    "            # If no detection, log 0\n",
    "            x_center, y_center = 0.0, 0.0\n",
    "\n",
    "        trajectory.append([frame_id, x_center, y_center])\n",
    "\n",
    "    # save to CSV\n",
    "    with open(csv_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"frame_id\", \"x_center\", \"y_center\"])\n",
    "        writer.writerows(trajectory)\n",
    "\n",
    "    print(f\"Trajectory saved to {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone-detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
